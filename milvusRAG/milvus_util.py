"""
Milvusæ•°æ®åº“å·¥å…·ç±»

åŠŸèƒ½ï¼š
1. è¿æ¥å’Œç®¡ç†Milvusæ•°æ®åº“
2. åˆå§‹åŒ–Embeddingæ¨¡å‹
3. åˆ›å»ºå’Œç®¡ç†å‘é‡å­˜å‚¨
4. æ–‡æ¡£æ·»åŠ å’Œç›¸ä¼¼æ€§æœç´¢
5. é›†åˆç®¡ç†
6. RAGå·¥ä½œæµé›†æˆ

è®¾è®¡ç†å¿µï¼š
- å°è£…ä¸ºå·¥å…·ç±»ï¼Œä¾¿äºRAGç³»ç»Ÿé›†æˆ
- æ”¯æŒçµæ´»é…ç½®ï¼ˆæ•°æ®åº“ã€é›†åˆã€ç´¢å¼•å‚æ•°ç­‰ï¼‰
- è‡ªåŠ¨ç®¡ç†è¿æ¥å’Œèµ„æº
- æä¾›æ¸…æ™°çš„APIæ¥å£
- èŒè´£åˆ†ç¦»ï¼Œæ¨¡å—åŒ–è®¾è®¡

å¤‡æ³¨ï¼š
1. æ–‡æœ¬åµŒå…¥æ¨¡å‹ä½¿ç”¨æœ¬åœ°ollamaæœåŠ¡
2. å¤§è¯­è¨€æ¨¡å‹ä½¿ç”¨ollamaäº‘æœåŠ¡(å…è´¹api)
"""

import logging
import os
from typing import Any, Dict, List, Literal, Optional

from IPython.display import Image, display
from langchain_classic.tools.retriever import create_retriever_tool
from langchain_milvus import Milvus
from langchain_ollama import ChatOllama, OllamaEmbeddings
from langgraph.graph import END, START, MessagesState, StateGraph
from langgraph.graph.state import CompiledStateGraph
from langgraph.prebuilt import ToolNode, tools_condition
from pymilvus import Collection, MilvusException, connections, db, utility

# ==================== å¸¸é‡å®šä¹‰ ====================

# é»˜è®¤é…ç½®
DEFAULT_HOST = "127.0.0.1"
DEFAULT_PORT = 19530
DEFAULT_DB_NAME = "milvus_demo"
DEFAULT_EMBEDDING_MODEL = "qwen3-embedding:4b"
DEFAULT_EMBEDDING_BASE_URL = "http://localhost:11434"
DEFAULT_LLM_MODEL = "deepseek-v3.1:671b"
DEFAULT_LLM_BASE_URL = "https://ollama.com"
DEFAULT_TEMPERATURE = 0.7

# ç´¢å¼•é…ç½®
DEFAULT_INDEX_TYPE = "HNSW"
DEFAULT_METRIC_TYPE = "L2"
DEFAULT_HNSW_M = 16
DEFAULT_HNSW_EF_CONSTRUCTION = 200
DEFAULT_HNSW_EF = 64

# æœç´¢é…ç½®
DEFAULT_SEARCH_K = 3
DEFAULT_MMR_FETCH_K = 20
DEFAULT_MMR_LAMBDA = 0.5

# Prompt æ¨¡æ¿
GRADE_PROMPT_TEMPLATE = (
    "ä½ æ˜¯ä¸€åè¯„å®¡å‘˜ï¼Œéœ€è¦åˆ¤æ–­æ£€ç´¢åˆ°çš„æ–‡æ¡£ä¸ç”¨æˆ·é—®é¢˜çš„ç›¸å…³æ€§ã€‚\n\n"
    "æ£€ç´¢åˆ°çš„æ–‡æ¡£ï¼š\n{context}\n\n"
    "ç”¨æˆ·é—®é¢˜ï¼š{question}\n\n"
    "è¯·åˆ¤æ–­æ–‡æ¡£æ˜¯å¦ä¸é—®é¢˜ç›¸å…³ã€‚å¦‚æœæ–‡æ¡£åŒ…å«ä¸é—®é¢˜ç›¸å…³çš„å…³é”®è¯æˆ–è¯­ä¹‰ä¿¡æ¯ï¼Œåˆ¤å®šä¸ºç›¸å…³ã€‚\n"
    "åªéœ€å›ç­” 'yes' æˆ– 'no'ï¼Œä¸è¦æœ‰å…¶ä»–å†…å®¹ã€‚"
)

REWRITE_PROMPT_TEMPLATE = (
    "è¯·å®¡è§†è¾“å…¥å†…å®¹ï¼Œå¹¶å°½é‡æ¨ç†å…¶æ½œåœ¨çš„è¯­ä¹‰æ„å›¾ã€‚\n"
    "è¿™æ˜¯æœ€åˆçš„é—®é¢˜ï¼š"
    "\n ------- \n"
    "{question}"
    "\n ------- \n"
    "è¯·å°†å…¶æ”¹å†™ä¸ºæ›´ä¼˜çš„é—®é¢˜ï¼š"
)

GENERATE_PROMPT_TEMPLATE = (
    "ä½ æ˜¯ä¸€åé—®ç­”åŠ©æ‰‹ã€‚è¯·åˆ©ç”¨ä»¥ä¸‹æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ç‰‡æ®µæ¥å›ç­”é—®é¢˜ã€‚"
    "å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆï¼Œå°±ç›´æ¥è¯´ä½ ä¸çŸ¥é“ã€‚"
    "ç­”æ¡ˆå›å¤å°½é‡è¯¦ç»†ï¼Œä¸è¦è¿‡äºç®€æ´ã€‚\n"
    "é—®é¢˜: {question} \n"
    "ä¸Šä¸‹æ–‡: {context}"
)

# RAG å·¥ä½œæµç›¸å…³é…ç½®
## å·¥å…·åç§°ï¼šç”¨äºæ£€ç´¢ç›¸å…³æ–‡æ¡£
RETRIEVER_TOOL_NAME = "retrieve_documents"
## å·¥å…·æè¿°ï¼šç”¨äºå·¥ä½œæµå·¥å…·é›†æˆæ–‡æ¡£æœç´¢åŠŸèƒ½
RETRIEVER_TOOL_DESCRIPTION = "æœç´¢å¹¶è¿”å›ç›¸å…³æ–‡æ¡£"
## é»˜è®¤å·¥ä½œæµå›¾ä¿å­˜è·¯å¾„
DEFAULT_GRAPH_PATH = "./workflow_graph.png"

# ç¯å¢ƒå˜é‡ ollama api key
if not os.environ.get("OLLAMA_API_KEY"):
    os.environ["OLLAMA_API_KEY"] = "xxxxx"

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)


class MilvusUtil:
    """
    Milvusæ•°æ®åº“å·¥å…·ç±»ï¼Œç”¨äºRAGç³»ç»Ÿçš„å‘é‡å­˜å‚¨

    ä¸»è¦åŠŸèƒ½ï¼š
    - æ•°æ®åº“è¿æ¥ç®¡ç†
    - å‘é‡å­˜å‚¨å’Œæ£€ç´¢
    - RAGå·¥ä½œæµé›†æˆ
    - æ–‡æ¡£ç®¡ç†å’Œæœç´¢
    """

    def __init__(
        self,
        host: str = DEFAULT_HOST,
        port: int = DEFAULT_PORT,
        db_name: str = DEFAULT_DB_NAME,
        embedding_model: str = DEFAULT_EMBEDDING_MODEL,
        embedding_base_url: str = DEFAULT_EMBEDDING_BASE_URL,
        llm_model: str = DEFAULT_LLM_MODEL,
        llm_base_url: str = DEFAULT_LLM_BASE_URL,
        collection_name: Optional[str] = None,
        drop_old: bool = False,
        verbose: bool = False,
    ):
        """
        åˆå§‹åŒ–Milvuså·¥å…·ç±»

        Args:
            host: MilvusæœåŠ¡å™¨åœ°å€
            port: MilvusæœåŠ¡å™¨ç«¯å£
            db_name: æ•°æ®åº“åç§°
            embedding_model: Embeddingæ¨¡å‹åç§°
            embedding_base_url: EmbeddingæœåŠ¡åœ°å€
            llm_model: LLMæ¨¡å‹åç§°
            llm_base_url: LLMæœåŠ¡åœ°å€
            collection_name: é›†åˆåç§°ï¼ˆå¯é€‰ï¼Œä¸æŒ‡å®šåˆ™ä½¿ç”¨é»˜è®¤ï¼‰
            drop_old: æ˜¯å¦åˆ é™¤å·²å­˜åœ¨çš„é›†åˆ
            verbose: æ˜¯å¦è¾“å‡ºè¯¦ç»†æ—¥å¿—
        """
        # æ•°æ®åº“é…ç½®
        self.host = host
        self.port = port
        self.db_name = db_name

        # Embeddingé…ç½®
        self.embedding_model = embedding_model
        self.embedding_base_url = embedding_base_url

        # LLMé…ç½®
        self.llm_model = llm_model
        self.llm_base_url = llm_base_url

        # é›†åˆé…ç½®
        self.collection_name = collection_name
        self.drop_old = drop_old

        # è¿è¡Œæ—¶é…ç½®
        self.verbose = verbose

        # å†…éƒ¨çŠ¶æ€
        self._connected = False
        self.embeddings: Optional[OllamaEmbeddings] = None
        self.vector_store: Optional[Milvus] = None
        self.llm: Optional[ChatOllama] = None
        self.retriever_tool = None

        # é…ç½®æ—¥å¿—çº§åˆ«
        if verbose:
            logging.basicConfig(level=logging.INFO)

    # ==================== è¿æ¥ç®¡ç† ====================

    def connect(self) -> bool:
        """
        è¿æ¥åˆ°Milvusæ•°æ®åº“

        Returns:
            bool: è¿æ¥æ˜¯å¦æˆåŠŸ
        """
        try:
            connections.connect(host=self.host, port=self.port)

            if connections.has_connection("default"):
                self._connected = True
                if self.verbose:
                    logger.info(f"âœ… æˆåŠŸè¿æ¥åˆ°Milvus: {self.host}:{self.port}")
                return True
            else:
                logger.error("âŒ è¿æ¥å¤±è´¥: æœªå»ºç«‹é»˜è®¤è¿æ¥")
                return False
        except Exception as e:
            logger.error(f"âŒ Milvusè¿æ¥å¤±è´¥: {e}")
            return False

    def close(self) -> None:
        """å…³é—­æ•°æ®åº“è¿æ¥"""
        try:
            if self._connected:
                connections.disconnect("default")
                self._connected = False
                if self.verbose:
                    logger.info("âœ… å·²æ–­å¼€Milvusè¿æ¥")
        except Exception as e:
            logger.error(f"âŒ æ–­å¼€è¿æ¥å¤±è´¥: {e}")

    # ==================== æ•°æ®åº“ç®¡ç† ====================

    def setup_database(self) -> bool:
        """
        è®¾ç½®æ•°æ®åº“ï¼ˆå­˜åœ¨åˆ™å¤ç”¨ï¼Œä¸å­˜åœ¨åˆ™åˆ›å»ºï¼‰

        Returns:
            bool: æ“ä½œæ˜¯å¦æˆåŠŸ
        """
        try:
            existing_databases = db.list_database()

            if self.db_name in existing_databases:
                if self.verbose:
                    logger.info(f"æ•°æ®åº“ '{self.db_name}' å·²å­˜åœ¨ï¼Œç›´æ¥ä½¿ç”¨")
            else:
                db.create_database(self.db_name)
                if self.verbose:
                    logger.info(f"âœ… åˆ›å»ºæ•°æ®åº“ '{self.db_name}'")

            # åˆ‡æ¢åˆ°æŒ‡å®šæ•°æ®åº“
            db.using_database(self.db_name)

            if self.verbose:
                collections = utility.list_collections()
                if collections:
                    logger.info(f"å½“å‰æ•°æ®åº“ä¸­çš„é›†åˆ: {collections}")

            return True
        except MilvusException as e:
            logger.error(f"âŒ æ•°æ®åº“æ“ä½œå¤±è´¥: {e}")
            return False

    # ==================== æ¨¡å‹åˆå§‹åŒ– ====================

    def init_embeddings(self) -> bool:
        """
        åˆå§‹åŒ–Embeddingæ¨¡å‹

        Returns:
            bool: åˆå§‹åŒ–æ˜¯å¦æˆåŠŸ
        """
        try:
            self.embeddings = OllamaEmbeddings(
                model=self.embedding_model,
                base_url=self.embedding_base_url,
            )

            if self.verbose:
                # æµ‹è¯•embedding
                test_embedding = self.embeddings.embed_query("æµ‹è¯•")
                logger.info(
                    f"âœ… Embeddingæ¨¡å‹åˆå§‹åŒ–æˆåŠŸï¼Œå‘é‡ç»´åº¦: {len(test_embedding)}"
                )

            return True
        except Exception as e:
            logger.error(f"âŒ Embeddingæ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            return False

    def init_response_model(
        self,
        model: Optional[str] = None,
        base_url: Optional[str] = None,
        temperature: float = DEFAULT_TEMPERATURE,
    ) -> bool:
        """
        åˆå§‹åŒ–å“åº”æ¨¡å‹ï¼ˆLLMï¼‰

        Args:
            model: æ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨åˆå§‹åŒ–æ—¶çš„é…ç½®ï¼‰
            base_url: æœåŠ¡åœ°å€ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨åˆå§‹åŒ–æ—¶çš„é…ç½®ï¼‰
            temperature: æ¸©åº¦å‚æ•°

        Returns:
            bool: åˆå§‹åŒ–æ˜¯å¦æˆåŠŸ
        """
        try:
            model_name = model or self.llm_model
            service_url = base_url or self.llm_base_url

            self.llm = ChatOllama(
                model=model_name,
                base_url=service_url,
                temperature=temperature,
                headers={"Authorization": f"Bearer {os.environ.get('OLLAMA_API_KEY')}"},
            )

            if self.verbose:
                logger.info(f"âœ… LLMæ¨¡å‹åˆå§‹åŒ–æˆåŠŸ: {model_name}")

            return True
        except Exception as e:
            logger.error(f"âŒ åˆå§‹åŒ–å“åº”æ¨¡å‹å¤±è´¥: {e}")
            return False

    # ==================== å‘é‡å­˜å‚¨ç®¡ç† ====================

    def create_vector_store(
        self,
        index_params: Optional[Dict[str, Any]] = None,
        search_params: Optional[Dict[str, Any]] = None,
    ) -> bool:
        """
        åˆ›å»ºå‘é‡å­˜å‚¨

        Args:
            index_params: ç´¢å¼•å‚æ•°ï¼ˆå¯é€‰ï¼‰
                - index_type: ç´¢å¼•ç±»å‹ ("HNSW", "IVF_FLAT", "AUTOINDEX")
                - metric_type: è·ç¦»åº¦é‡ ("L2", "IP", "COSINE")
                - params: ç´¢å¼•ç‰¹å®šå‚æ•°
            search_params: æœç´¢å‚æ•°ï¼ˆå¯é€‰ï¼‰
                - metric_type: è·ç¦»åº¦é‡æ–¹å¼
                - params: æœç´¢ç‰¹å®šå‚æ•°

        Returns:
            bool: åˆ›å»ºæ˜¯å¦æˆåŠŸ
        """
        if not self.embeddings:
            logger.error("âŒ è¯·å…ˆåˆå§‹åŒ–Embeddingæ¨¡å‹")
            return False

        try:
            # é»˜è®¤ç´¢å¼•å‚æ•°ï¼ˆHNSWï¼‰
            if index_params is None:
                index_params = {
                    "index_type": DEFAULT_INDEX_TYPE,
                    "metric_type": DEFAULT_METRIC_TYPE,
                    "params": {
                        "M": DEFAULT_HNSW_M,
                        "efConstruction": DEFAULT_HNSW_EF_CONSTRUCTION,
                    },
                }

            # é»˜è®¤æœç´¢å‚æ•°
            if search_params is None:
                search_params = {
                    "metric_type": DEFAULT_METRIC_TYPE,
                    "params": {
                        "ef": DEFAULT_HNSW_EF,
                    },
                }

            # æ„å»ºè¿æ¥å‚æ•°
            connection_args = {
                "uri": f"http://{self.host}:{self.port}",
                "token": "root:Milvus",
                "db_name": self.db_name,
            }

            # åˆ›å»ºå‘é‡å­˜å‚¨
            kwargs = {
                "embedding_function": self.embeddings,
                "connection_args": connection_args,
                "index_params": index_params,
                "search_params": search_params,
                "consistency_level": "Strong",
                "drop_old": self.drop_old,
            }

            # å¦‚æœæŒ‡å®šäº†é›†åˆåç§°ï¼Œæ·»åŠ åˆ°å‚æ•°ä¸­
            if self.collection_name:
                kwargs["collection_name"] = self.collection_name

            self.vector_store = Milvus(**kwargs)

            if self.verbose:
                logger.info("âœ… å‘é‡å­˜å‚¨åˆ›å»ºæˆåŠŸ")

            return True
        except Exception as e:
            logger.error(f"âŒ å‘é‡å­˜å‚¨åˆ›å»ºå¤±è´¥: {e}")
            return False

    # ==================== æ–‡æ¡£ç®¡ç† ====================

    def add_texts(
        self, texts: List[str], metadatas: Optional[List[Dict[str, Any]]] = None
    ) -> bool:
        """
        æ·»åŠ æ–‡æœ¬åˆ°å‘é‡å­˜å‚¨

        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            metadatas: å…ƒæ•°æ®åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰

        Returns:
            bool: æ·»åŠ æ˜¯å¦æˆåŠŸ
        """
        if not self.vector_store:
            logger.error("âŒ è¯·å…ˆåˆ›å»ºå‘é‡å­˜å‚¨")
            return False

        try:
            self.vector_store.add_texts(texts, metadatas=metadatas)

            if self.verbose:
                logger.info(f"âœ… æˆåŠŸæ·»åŠ  {len(texts)} æ¡æ–‡æ¡£")

            return True
        except Exception as e:
            logger.error(f"âŒ æ·»åŠ æ–‡æ¡£å¤±è´¥: {e}")
            return False

    def add_documents(self, documents: List[Any]) -> bool:
        """
        æ·»åŠ Documentå¯¹è±¡åˆ°å‘é‡å­˜å‚¨ï¼ˆæ¨èç”¨äºRAGåœºæ™¯ï¼‰

        Args:
            documents: Documentå¯¹è±¡åˆ—è¡¨ï¼ˆæ¥è‡ªLangChainçš„æ–‡æ¡£åŠ è½½å™¨ï¼‰

        Returns:
            bool: æ·»åŠ æ˜¯å¦æˆåŠŸ
        """
        if not self.vector_store:
            logger.error("âŒ è¯·å…ˆåˆ›å»ºå‘é‡å­˜å‚¨")
            return False

        try:
            self.vector_store.add_documents(documents)

            if self.verbose:
                logger.info(f"âœ… æˆåŠŸæ·»åŠ  {len(documents)} æ¡æ–‡æ¡£")
                # ç»Ÿè®¡æ–‡æ¡£æ¥æº
                sources = set()
                for doc in documents:
                    if hasattr(doc, "metadata") and "source" in doc.metadata:
                        sources.add(doc.metadata["source"])
                if sources:
                    logger.info(f"æ–‡æ¡£æ¥æº: {len(sources)} ä¸ªæ–‡ä»¶")

            return True
        except Exception as e:
            logger.error(f"âŒ æ·»åŠ æ–‡æ¡£å¤±è´¥: {e}")
            return False

    # ==================== æ£€ç´¢å’Œæœç´¢ ====================

    def similarity_search(
        self,
        query: str,
        k: int = DEFAULT_SEARCH_K,
        filter: Optional[Dict[str, Any]] = None,
    ) -> CompiledStateGraph:
        """
        ç›¸ä¼¼åº¦æœç´¢å¹¶åˆ›å»ºRAGå·¥ä½œæµ

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            k: è¿”å›ç»“æœæ•°é‡
            filter: è¿‡æ»¤æ¡ä»¶ï¼ˆå¯é€‰ï¼‰

        Returns:
            CompiledStateGraph: ç¼–è¯‘åçš„å·¥ä½œæµå›¾ï¼Œå¤±è´¥è¿”å›None
        """
        if not self.vector_store:
            logger.error("âŒ è¯·å…ˆåˆ›å»ºå‘é‡å­˜å‚¨")
            return None

        try:
            # results = self.vector_store.similarity_search(query, k=k, expr=filter)
            retriever = self.vector_store.as_retriever(search_kwargs={"k": k})
            self.retriever_tool = create_retriever_tool(
                retriever,
                RETRIEVER_TOOL_NAME,
                RETRIEVER_TOOL_DESCRIPTION,
            )

            graph = self._build_graph()

            return graph
        except Exception as e:
            logger.error(f"âŒ æœç´¢å¤±è´¥: {e}")
            return None

    def mmr_search(
        self,
        query: str,
        k: int = DEFAULT_SEARCH_K,
        fetch_k: int = DEFAULT_MMR_FETCH_K,
        lambda_mult: float = DEFAULT_MMR_LAMBDA,
    ) -> List[Any]:
        """
        æœ€å¤§è¾¹é™…ç›¸å…³æ€§æœç´¢ï¼ˆMMRï¼‰ï¼Œå‡å°‘å†—ä½™ã€æå‡å¤šæ ·æ€§

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            k: è¿”å›ç»“æœæ•°é‡
            fetch_k: åˆæ­¥æ£€ç´¢çš„æ–‡æ¡£æ•°é‡
            lambda_mult: å¤šæ ·æ€§å‚æ•°ï¼ˆ0-1ï¼‰ï¼Œè¶Šå°è¶Šå¤šæ ·

        Returns:
            List: æœç´¢ç»“æœåˆ—è¡¨
        """
        if not self.vector_store:
            logger.error("âŒ è¯·å…ˆåˆ›å»ºå‘é‡å­˜å‚¨")
            return []

        try:
            results = self.vector_store.max_marginal_relevance_search(
                query, k=k, fetch_k=fetch_k, lambda_mult=lambda_mult
            )
            # retriever = self.vector_store.as_retriever(
            #     search_type="mmr",
            #     search_kwargs={
            #         "k": k,
            #         "fetch_k": fetch_k,
            #         "lambda_mult": lambda_mult,
            #     },
            # )
            # results = retriever.invoke(query)

            if self.verbose:
                logger.info(f"MMRæœç´¢æ‰¾åˆ° {len(results)} æ¡å¤šæ ·åŒ–æ–‡æ¡£")

            return results
        except Exception as e:
            logger.error(f"âŒ MMRæœç´¢å¤±è´¥: {e}")
            return []

    # ==================== é›†åˆç®¡ç† ====================

    def get_collection_info(self) -> Optional[Dict[str, Any]]:
        """
        è·å–é›†åˆä¿¡æ¯

        Returns:
            Dict: é›†åˆä¿¡æ¯å­—å…¸ï¼Œå¤±è´¥è¿”å›None
        """
        try:
            collections = utility.list_collections()

            if not collections:
                logger.info("å½“å‰æ•°æ®åº“æ²¡æœ‰é›†åˆ")
                return None

            # è·å–ç¬¬ä¸€ä¸ªé›†åˆçš„ä¿¡æ¯ï¼ˆæˆ–æŒ‡å®šçš„é›†åˆï¼‰
            target_collection = (
                self.collection_name if self.collection_name else collections[0]
            )

            if target_collection not in collections:
                logger.warning(f"é›†åˆ '{target_collection}' ä¸å­˜åœ¨")
                return None

            collection = Collection(name=target_collection)
            collection.load()

            info = {
                "name": target_collection,
                "is_empty": collection.is_empty,
                "description": collection.description,
            }

            # è·å–å‘é‡ç»´åº¦
            for field in collection.schema.fields:
                if field.dtype.name == "FLOAT_VECTOR":
                    info["vector_dim"] = field.params.get("dim")
                    break

            # è·å–ç´¢å¼•ä¿¡æ¯
            indexes = collection.indexes
            info["indexes"] = [
                {"field": idx.field_name, "params": idx.params} for idx in indexes
            ]

            return info
        except Exception as e:
            logger.error(f"âŒ è·å–é›†åˆä¿¡æ¯å¤±è´¥: {e}")
            return None

    def drop_collection(self, collection_name: Optional[str] = None) -> bool:
        """
        åˆ é™¤é›†åˆ

        Args:
            collection_name: é›†åˆåç§°ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨å½“å‰é›†åˆï¼‰

        Returns:
            bool: åˆ é™¤æ˜¯å¦æˆåŠŸ
        """
        try:
            target_name = collection_name or self.collection_name

            if not target_name:
                logger.error("âŒ æœªæŒ‡å®šé›†åˆåç§°")
                return False

            collections = utility.list_collections()

            if target_name not in collections:
                logger.warning(f"é›†åˆ '{target_name}' ä¸å­˜åœ¨")
                return False

            collection = Collection(name=target_name)
            collection.drop()

            if self.verbose:
                logger.info(f"âœ… æˆåŠŸåˆ é™¤é›†åˆ '{target_name}'")

            return True
        except Exception as e:
            logger.error(f"âŒ åˆ é™¤é›†åˆå¤±è´¥: {e}")
            return False

    # ==================== RAG å·¥ä½œæµ ====================

    def generate_query_or_respond(self, state: MessagesState) -> Dict[str, List]:
        """
        ç”ŸæˆæŸ¥è¯¢æˆ–ç›´æ¥å“åº”

        ä¼˜å…ˆè°ƒç”¨æ£€ç´¢å·¥å…·æ£€ç´¢ï¼›è‹¥æœªæ£€ç´¢åˆ°ç›¸å…³ç»“æœåˆ™è¿”å›'æœªæ‰¾åˆ°ç›¸å…³å†…å®¹'çš„å›å¤ã€‚

        Args:
            state: æ¶ˆæ¯çŠ¶æ€

        Returns:
            Dict: æ›´æ–°åçš„æ¶ˆæ¯å­—å…¸
        """
        response = self.llm.bind_tools([self.retriever_tool]).invoke(state["messages"])
        return {"messages": [response]}

    def grade_documents(
        self,
        state: MessagesState,
    ) -> Literal["generate_answer", "rewrite_question"]:
        """
        è¯„ä¼°æ£€ç´¢æ–‡æ¡£çš„ç›¸å…³æ€§

        åˆ¤æ–­æ£€ç´¢åˆ°çš„æ–‡æ¡£æ˜¯å¦ä¸è¯¥é—®é¢˜ç›¸å…³ã€‚

        Args:
            state: æ¶ˆæ¯çŠ¶æ€

        Returns:
            str: ä¸‹ä¸€æ­¥æ“ä½œï¼ˆ"generate_answer" æˆ– "rewrite_question"ï¼‰
        """
        question = state["messages"][0].content
        context = state["messages"][-1].content

        prompt = GRADE_PROMPT_TEMPLATE.format(question=question, context=context)
        response = self.llm.invoke([{"role": "user", "content": prompt}])

        # æå–å“åº”å†…å®¹å¹¶è½¬æ¢ä¸ºå°å†™è¿›è¡Œåˆ¤æ–­
        score = response.content.strip().lower()

        if self.verbose:
            logger.info(f"ğŸ“Š æ–‡æ¡£ç›¸å…³æ€§è¯„åˆ†: {score}")

        if "yes" in score:
            return "generate_answer"
        else:
            return "rewrite_question"

    def rewrite_question(self, state: MessagesState) -> Dict[str, List]:
        """
        é‡å†™ç”¨æˆ·é—®é¢˜

        å¯¹åŸå§‹é—®é¢˜è¿›è¡Œæ”¹å†™ä»¥æé«˜æ£€ç´¢æ•ˆæœã€‚

        Args:
            state: æ¶ˆæ¯çŠ¶æ€

        Returns:
            Dict: åŒ…å«é‡å†™åé—®é¢˜çš„æ¶ˆæ¯å­—å…¸
        """
        messages = state["messages"]
        question = messages[0].content
        prompt = REWRITE_PROMPT_TEMPLATE.format(question=question)
        response = self.llm.invoke([{"role": "user", "content": prompt}])
        return {"messages": [{"role": "user", "content": response.content}]}

    def generate_answer(self, state: MessagesState) -> Dict[str, List]:
        """
        ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ

        åŸºäºæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ç”Ÿæˆå›ç­”ã€‚

        Args:
            state: æ¶ˆæ¯çŠ¶æ€

        Returns:
            Dict: åŒ…å«ç”Ÿæˆç­”æ¡ˆçš„æ¶ˆæ¯å­—å…¸
        """
        question = state["messages"][0].content
        context = state["messages"][-1].content
        prompt = GENERATE_PROMPT_TEMPLATE.format(question=question, context=context)
        response = self.llm.invoke([{"role": "user", "content": prompt}])
        return {"messages": [response]}

    def _build_graph(
        self, save_path: Optional[str] = DEFAULT_GRAPH_PATH
    ) -> CompiledStateGraph:
        """
        æ„å»ºRAGå·¥ä½œæµå›¾ï¼ˆç§æœ‰æ–¹æ³•ï¼‰

        åˆ›å»ºåŒ…å«æ£€ç´¢ã€è¯„ä¼°ã€é‡å†™å’Œç­”æ¡ˆç”Ÿæˆçš„å®Œæ•´å·¥ä½œæµã€‚

        Args:
            save_path: å·¥ä½œæµå›¾ä¿å­˜è·¯å¾„ï¼ˆå¯é€‰ï¼‰

        Returns:
            CompiledStateGraph: ç¼–è¯‘åçš„å·¥ä½œæµå›¾
        """
        workflow = StateGraph(MessagesState)

        # å®šä¹‰å·¥ä½œæµä¸­ä¼šåˆ‡æ¢çš„èŠ‚ç‚¹
        workflow.add_node(self.generate_query_or_respond)
        workflow.add_node("retrieve", ToolNode([self.retriever_tool]))
        workflow.add_node(self.rewrite_question)
        workflow.add_node(self.generate_answer)

        # è®¾ç½®èµ·å§‹è¾¹
        workflow.add_edge(START, "generate_query_or_respond")

        # åˆ¤æ–­æ˜¯å¦éœ€è¦æ£€ç´¢
        workflow.add_conditional_edges(
            "generate_query_or_respond",
            tools_condition,  # åˆ¤æ–­LLMçš„å†³ç­–ï¼ˆè°ƒç”¨å·¥å…·è¿˜æ˜¯ç›´æ¥å›å¤ï¼‰
            {
                "tools": "retrieve",  # éœ€è¦æ£€ç´¢
                END: END,  # ç›´æ¥ç»“æŸ
            },
        )

        # æ£€ç´¢åè¯„ä¼°æ–‡æ¡£ç›¸å…³æ€§
        workflow.add_conditional_edges(
            "retrieve",
            self.grade_documents,  # è¯„ä¼°æ–‡æ¡£ç›¸å…³æ€§
        )

        # è®¾ç½®å…¶ä»–è¾¹
        workflow.add_edge("generate_answer", END)
        workflow.add_edge("rewrite_question", "generate_query_or_respond")

        # ç¼–è¯‘å·¥ä½œæµ
        graph = workflow.compile()

        # å°è¯•ä¿å­˜å·¥ä½œæµå›¾
        self._save_graph_image(graph, save_path)

        return graph

    def _save_graph_image(
        self, graph: CompiledStateGraph, save_path: Optional[str] = None
    ) -> None:
        """
        ä¿å­˜å·¥ä½œæµå›¾ä¸ºå›¾ç‰‡ï¼ˆç§æœ‰æ–¹æ³•ï¼‰

        Args:
            graph: ç¼–è¯‘åçš„å·¥ä½œæµå›¾
            save_path: ä¿å­˜è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        """
        try:
            png_data = graph.get_graph().draw_mermaid_png()

            if save_path:
                with open(save_path, "wb") as f:
                    f.write(png_data)
                if self.verbose:
                    logger.info(f"âœ… å·¥ä½œæµå›¾å·²ä¿å­˜åˆ°: {save_path}")
            else:
                # å°è¯•åœ¨ Jupyter ä¸­æ˜¾ç¤º
                try:
                    display(Image(png_data))
                except NameError:
                    # å¦‚æœä¸åœ¨ Jupyter ç¯å¢ƒä¸­ï¼Œä¿å­˜åˆ°é»˜è®¤ä½ç½®
                    default_path = DEFAULT_GRAPH_PATH
                    with open(default_path, "wb") as f:
                        f.write(png_data)
                    if self.verbose:
                        logger.info(f"ğŸ“Š å·¥ä½œæµå›¾å·²ä¿å­˜åˆ°: {default_path}")
        except Exception as e:
            logger.warning(f"âš ï¸ æ— æ³•ç”Ÿæˆå·¥ä½œæµå›¾: {e}")

    # ==================== åˆå§‹åŒ–å’Œèµ„æºç®¡ç† ====================

    def initialize(self) -> bool:
        """
        ä¸€é”®åˆå§‹åŒ–æ‰€æœ‰ç»„ä»¶

        æ‰§è¡Œä»¥ä¸‹åˆå§‹åŒ–æ­¥éª¤ï¼š
        1. è¿æ¥Milvusæ•°æ®åº“
        2. è®¾ç½®/åˆ›å»ºæ•°æ®åº“
        3. åˆå§‹åŒ–Embeddingæ¨¡å‹
        4. åˆ›å»ºå‘é‡å­˜å‚¨
        5. åˆå§‹åŒ–LLMå“åº”æ¨¡å‹

        Returns:
            bool: åˆå§‹åŒ–æ˜¯å¦æˆåŠŸ
        """
        steps = [
            ("è¿æ¥æ•°æ®åº“", self.connect),
            ("è®¾ç½®æ•°æ®åº“", self.setup_database),
            ("åˆå§‹åŒ–Embeddingæ¨¡å‹", self.init_embeddings),
            ("åˆ›å»ºå‘é‡å­˜å‚¨", self.create_vector_store),
            ("åˆå§‹åŒ–å“åº”æ¨¡å‹", self.init_response_model),
        ]

        for step_name, step_func in steps:
            if not step_func():
                logger.error(f"âŒ åˆå§‹åŒ–å¤±è´¥äºæ­¥éª¤: {step_name}")
                return False

        if self.verbose:
            logger.info("ğŸ‰ Milvuså·¥å…·ç±»åˆå§‹åŒ–å®Œæˆ")

        return True
